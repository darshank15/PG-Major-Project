{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/vatsal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/vatsal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from nltk.corpus import wordnet\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer \n",
    "import gensim \n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entry(line): \n",
    "    w, c = line.split(\"\\t\", 2)\n",
    "    return (w, int(c))\n",
    "\n",
    "dict_path = \"./dict.txt\"\n",
    "dictionary = dict(entry(line) for line in open(dict_path))\n",
    "max_word_length = max(map(len, dictionary))\n",
    "total = float(sum(dictionary.values()))\n",
    "cleanup = re.compile(r'[^a-z0-9]')\n",
    "\n",
    "def word_prob(word): \n",
    "    return dictionary.get(word, 0) / total\n",
    "\n",
    "def segment(text): \n",
    "    text = re.sub(cleanup, ' ', text)\n",
    "    probs, lasts = [1.0], [0]\n",
    "    for i in range(1,len(text) + 1):\n",
    "        prob_k, k = max((probs[j] * word_prob(text[j:i]), j)for j in range(max(0, i - max_word_length), i))\n",
    "        probs.append(prob_k)\n",
    "        lasts.append(k)\n",
    "    words = []\n",
    "    i = len(text)\n",
    "    while i > 0:\n",
    "        words.append(text[lasts[i]:i])\n",
    "        i = lasts[i]\n",
    "    words.reverse()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Corpus, Glove\n",
    "corpus = Corpus() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100 = {}\n",
    "top100['travel'] = ['#travel', '#wanderlust', '#nature', '#travelling', '#traveling', '#traveller', '#photography', '#traveler', '#trip', '#travels', '#vacation', '#love', '#travelers', '#adventure', '#tourist', '#landscape', '#travellers', '#explore', '#holiday', '#beautiful', '#tourism', '#hiking', '#beach', '#photo', '#sunset', '#photographer', '#mountains', '#globetrotter', '#summer', '#art', '#sky', '#treking', '#europe', '#view', '#architecture', '#sea', '#happy', '#fun', '#city', '#sun', '#lifestyle', '#amazing', '#wanderer', '#italy', '#follow', '#backpacking', '#life', '#visiting', '#fashion', '#autumn', '#explorer', '#ocean', '#outdoors', '#india', '#world', '#mountain', '#beauty', '#spain', '#backpacker', '#style', '#like', '#france', '#exploring', '#trekking', '#clouds', '#asia', '#me', '#friends', '#usa', '#canon', '#happiness', '#blogger', '#holidays', '#ig', '#sunrise', '#smile', '#germany', '#girl', '#island', '#wander', '#paradise', '#turkey', '#discover', '#italia', '#voyage', '#flowers', '#landscapes', '#sightseeing', '#outdoor', '#history', '#indonesia', '#cute', '#forest', '#paris', '#food', '#australia', '#bali', '#pic', '#beaches', '#inspiration']\n",
    "top100['food'] = ['#foodie', '#food', '#delicious', '#yummy', '#foodies', '#foods', '#eat', '#breakfast', '#dinner', '#tasty', '#cooking', '#lunch', '#homemade', '#dessert', '#love', '#eating', '#healthy', '#sweet', '#restaurant', '#photography', '#hungry', '#chef', '#blogger', '#travel', '#chocolate', '#baking', '#follow', '#cake', '#vegan', '#fresh', '#chicken', '#like', '#cook', '#amazing', '#blog', '#happy', '#favorite', '#brunch', '#coffee', '#weekend', '#fit', '#vegetarian', '#pasta', '#beautiful', '#pastry', '#fitness', '#gourmet', '#morning', '#desserts', '#seafood', '#eater', '#lifestyle', '#sweets', '#cafe', '#recipes', '#icecream', '#pizza', '#italy', '#culinary', '#candy', '#cheese', '#photographer', '#photo', '#cakes', '#recipe', '#noodles', '#diet', '#eats', '#rice', '#friends', '#cuisine', '#porridge', '#salad', '#nutrition', '#india', '#indonesia', '#bread', '#mornings', '#drinks', '#art', '#life', '#fun', '#gastronomy', '#sunday', '#cookies', '#kitchen', '#gym', '#me', '#beef', '#sushi', '#cupcakes', '#bake', '#spicy', '#saturday', '#fish', '#catering', '#burger', '#snack', '#music', '#delhi']\n",
    "top100['baby'] = ['#baby', '#kids', '#love', '#babies', '#cute', '#family', '#mom', '#fashion', '#newborn', '#children', '#happy', '#beautiful', '#photography', '#girl', '#babys', '#adorable', '#sweet', '#motherhood', '#pregnancy', '#handmade', '#funny', '#lovely', '#smile', '#mommy', '#life', '#daughter', '#style', '#follow', '#sweetheart', '#like', '#pregnant', '#beauty', '#precious', '#kid', '#child', '#fun', '#girls', '#mummy', '#tiny', '#twins', '#amazing', '#toddler', '#model', '#photo', '#mama', '#toys', '#boy', '#sleep', '#matching', '#autumn', '#enjoy', '#friends', '#pretty', '#art', '#parenting', '#live', '#mother', '#me', '#amor', '#photographer', '#princess', '#fairy', '#sale', '#happiness', '#boys', '#boutique', '#lifestyle', '#portrait', '#fall', '#son', '#pink', '#nature', '#childhood', '#cuteness', '#maternity', '#cool', '#travel', '#angel', '#best', '#halloween', '#play', '#weekend', '#october', '#dad', '#innocent', '#brand', '#makeup', '#home', '#parenthood', '#nice', '#little', '#sunday', '#flowers', '#mum', '#canon', '#infant', '#party', '#wedding', '#summer', '#hot']\n",
    "top100['jewellery'] = ['#jewellery', '#jewelry', '#fashion', '#accessories', '#earrings', '#necklace', '#style', '#jewels', '#handmade', '#beautiful', '#gemstone', '#trendy', '#gold', '#jewel', '#bracelet', '#rings', '#love', '#gems', '#silver', '#bracelets', '#design', '#ring', '#gemstones', '#crystals', '#shopping', '#stylish', '#art', '#pendant', '#diamond', '#cute', '#luxury', '#necklaces', '#bling', '#india', '#watches', '#handcrafted', '#designer', '#bijoux', '#trending', '#earring', '#bangles', '#bride', '#wedding', '#beauty', '#stone', '#tourmaline', '#charm', '#traditional', '#hyderabad', '#wholesaler', '#diamonds', '#bridal', '#photography', '#accessory', '#cabochons', '#tourmalines', '#stones', '#gem', '#goldplated', '#semiprecious', '#mumbai', '#silversmith', '#pendents', '#k', '#indian', '#sapphire', '#chennai', '#jewellers', '#girls', '#sale', '#jeweller', '#delhi', '#follow', '#fashionable', '#gift', '#bangalore', '#women', '#london', '#unique', '#ethnic', '#rawalpindi', '#crystal', '#vintage', '#oxidised', '#emerald', '#dubai', '#tucson', '#peridot', '#beads', '#makeup', '#saree', '#kunzite', '#chic', '#firecracker', '#pearl', '#lahore', '#choker', '#islamabad', '#wholesale', '#combo']\n",
    "# top100['selfie'] = ['#me', '#love', '#follow', '#smile', '#photography', '#style', '#beautiful', '#fashion', '#happy', '#cute', '#fun', '#girl', '#life', '#portrait', '#like', '#pretty', '#photo', '#handsome', '#travel', '#nature', '#eyes', '#face', '#hair', '#summer', '#art', '#friends', '#amazing', '#model', '#food', '#beauty', '#pic', '#daily', '#girls', '#lifestyle', '#look', '#makeup', '#swag', '#photographer', '#fitness', '#autumn', '#cool', '#family', '#sky', '#picture', '#likes', '#sun', '#mood', '#music', '#followers', '#outfit', '#boy', '#colorful', '#self', '#motivation', '#lovely', '#gay', '#pose', '#landscape', '#black', '#inspiration', '#happiness', '#halloween', '#artist', '#gym', '#liker', '#pink', '#portraits', '#blogger', '#fit', '#sunset', '#wanderlust', '#design', '#funny', '#italy', '#traveling', '#hot', '#awesome', '#holiday', '#following', '#photos', '#india', '#night', '#blue', '#weekend', '#view', '#october', '#party', '#trip', '#red', '#tattoo', '#man', '#looks', '#stylish', '#sea', '#nice', '#good', '#beard', '#beach', '#woman', '#aesthetic']\n",
    "top100['pet'] = ['#pet', '#pets', '#dog', '#cute', '#animals', '#love', '#dogs', '#cat', '#puppy', '#animal', '#cats', '#kitten', '#nature', '#kitty', '#meow', '#adorable', '#puppies', '#kittens', '#doggy', '#doggo', '#pup', '#photography', '#happy', '#funny', '#hound', '#eyes', '#beautiful', '#bunny', '#poodle', '#chihuahua', '#follow', '#rabbit', '#fluffy', '#paws', '#family', '#photo', '#lovely', '#chat', '#furry', '#daily', '#pomeranian', '#labrador', '#fun', '#autumn', '#sweet', '#maltese', '#smile', '#life', '#art', '#bunnies', '#hamster', '#baby', '#bird', '#me', '#world', '#bulldog', '#like', '#sleeping', '#tuesday', '#summer', '#parrots', '#husky', '#amor', '#rabbits', '#tot', '#zoo', '#doggie', '#meme', '#pug', '#istanbul', '#wildlife', '#beauty', '#adopt', '#girl', '#meowed', '#birds', '#purr', '#rescue', '#cavy', '#japan', '#doggies', '#halloween', '#photographer', '#woof', '#playtime', '#nice', '#portrait', '#relax', '#parakeet', '#singapore', '#parrot', '#fashion', '#fall', '#chihuahuas', '#friends', '#travel', '#morning', '#wolf', '#sweden', '#monday']\n",
    "top100['art'] = ['#art', '#artist', '#drawing', '#artwork', '#illustration', '#sketch', '#painting', '#draw', '#sketchbook', '#creative', '#design', '#love', '#photography', '#beautiful', '#pencil', '#ink', '#illustrator', '#drawings', '#portrait', '#arts', '#gallery', '#artistic', '#fashion', '#paint', '#doodle', '#nature', '#picture', '#watercolor', '#graphic', '#color', '#photo', '#sketching', '#music', '#style', '#pen', '#anime', '#cute', '#paintings', '#follow', '#paper', '#graphics', '#artists', '#like', '#travel', '#abstract', '#tattoo', '#cartoon', '#girl', '#masterpiece', '#sketches', '#beauty', '#inspiration', '#procreate', '#designer', '#comics', '#artworks', '#photographer', '#life', '#digital', '#illustrations', '#model', '#handmade', '#watercolour', '#draws', '#happy', '#painter', '#black', '#colorful', '#colors', '#comic', '#fun', '#landscape', '#exhibition', '#cool', '#colour', '#architecture', '#aesthetic', '#acrylic', '#smile', '#graffiti', '#arty', '#tattoos', '#amazing', '#lifestyle', '#sculpture', '#blue', '#rap', '#halloween', '#inked', '#funny', '#fantasy', '#character', '#me', '#summer', '#singer', '#studio', '#logo', '#collage', '#doodles', '#pictures']\n",
    "top100['architecture'] = ['#architecture', '#design', '#art', '#photography', '#building', '#travel', '#architect', '#city', '#buildings', '#urban', '#beautiful', '#style', '#minimal', '#street', '#architectural', '#interior', '#skyscraper', '#perspective', '#abstract', '#lines', '#town', '#architectures', '#geometric', '#designer', '#composition', '#geometry', '#arts', '#cities', '#home', '#landscape', '#nature', '#photo', '#photographer', '#interiors', '#house', '#italy', '#love', '#cityscape', '#modern', '#decor', '#pattern', '#luxury', '#sky', '#construction', '#architects', '#inspiration', '#arch', '#traveling', '#minimalism', '#render', '#wanderlust', '#sketch', '#rendering', '#europe', '#lifestyle', '#history', '#autumn', '#view', '#decoration', '#light', '#trip', '#amazing', '#concrete', '#paris', '#sunset', '#facade', '#artist', '#modernism', '#contemporary', '#sun', '#london', '#travelling', '#furniture', '#ig', '#life', '#france', '#church', '#traveller', '#canon', '#pic', '#explore', '#picture', '#details', '#colors', '#traveler', '#vacation', '#tourism', '#italia', '#milano', '#clouds', '#engineering', '#fashion', '#germany', '#spain', '#villa', '#wood', '#3d', '#exterior', '#blue', '#artwork']\n",
    "top100['nature'] = ['#nature', '#photography', '#landscape', '#travel', '#beautiful', '#love', '#autumn', '#photo', '#sky', '#photographer', '#mountains', '#sunset', '#outdoors', '#clouds', '#art', '#flowers', '#beauty', '#tree', '#wanderlust', '#follow', '#amazing', '#adventure', '#trees', '#forest', '#view', '#explore', '#green', '#summer', '#life', '#sun', '#hiking', '#colors', '#wildlife', '#mountain', '#travelling', '#fashion', '#like', '#canon', '#trip', '#sea', '#ig', '#happy', '#water', '#natural', '#fall', '#outdoor', '#beach', '#style', '#traveling', '#fun', '#blue', '#landscapes', '#sunrise', '#flower', '#animals', '#lake', '#cute', '#picture', '#macro', '#me', '#river', '#earth', '#natures', '#friends', '#leaves', '#followers', '#vacation', '#plants', '#weather', '#smile', '#garden', '#india', '#girl', '#pic', '#november', '#photos', '#world', '#wild', '#birds', '#model', '#traveller', '#peace', '#lifestyle', '#bird', '#walk', '#animal', '#holiday', '#discover', '#trekking', '#portrait', '#morning', '#nice', '#sunday', '#ocean', '#sunsets', '#traveler', '#fitness', '#rain', '#italy', '#winter']\n",
    "\n",
    "for topic in top100.keys():\n",
    "    top100[topic] = [r.replace('#', '') for r in top100[topic]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_corpora = {}\n",
    "\n",
    "for topic in top100.keys():\n",
    "    glob_corpora[topic] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_mapping = {}\n",
    "for key in top100.keys():\n",
    "    top100_stemmed = [ps.stem(word) for word in top100[key]]\n",
    "    top100_mapping[key]={}\n",
    "    for i in range(0,100):\n",
    "        top100_mapping[key][top100_stemmed[i]]=top100[key][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punc(s):\n",
    "    new_str = \"\"\n",
    "    for c in s:\n",
    "        if c in punctuation:\n",
    "            new_str += \" \"\n",
    "        else:\n",
    "            new_str += c\n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture\n",
      "./jsons/architecture/modernarchitecture.json\n",
      "./jsons/architecture/architecture.json\n",
      "./jsons/architecture/architecturelovers.json\n",
      "./jsons/architecture/architecturephotography.json\n",
      "./jsons/architecture/architecture_hunter.json\n",
      "./jsons/architecture/architectureporn.json\n",
      "5331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic in top100.keys():\n",
    "    for r, d, f in os.walk(\"./jsons/\"+topic):\n",
    "        print(topic)\n",
    "        df = pd.read_csv('./csv/train/final_train_'+topic+\".csv\")\n",
    "        dic = dict()\n",
    "        for file in f:\n",
    "            path = r+\"/\"+file\n",
    "            print(path)\n",
    "            if '.json' in file:\n",
    "                file_ptr = open(path, \"r\")\n",
    "                tmp_dic = json.load(file_ptr)\n",
    "                dic.update(tmp_dic)\n",
    "                \n",
    "        print(len(dic))\n",
    "        for index,row in df.iterrows():\n",
    "            p_id = row['post_id'].split('.')[0]\n",
    "            try:\n",
    "                text = strip_punc(dic[p_id]['text_des'].lower())\n",
    "            except:\n",
    "                continue\n",
    "            toks = word_tokenize(text)\n",
    "            toks_ = []\n",
    "            for tok in toks:\n",
    "                tok = segment(tok)\n",
    "                for t in tok:\n",
    "                    if not wordnet.synsets(t) or len(t) < 3:\n",
    "                        continue\n",
    "\n",
    "                    t = ps.stem(t)\n",
    "                    if t not in stop_words:\n",
    "                        toks_.append(t)\n",
    "\n",
    "            if toks_:\n",
    "                for topic in top100.keys():\n",
    "                    if topic in path:\n",
    "                        glob_corpora[topic].append(toks_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for r, d, f in os.walk(\"./jsons\"):\n",
    "#     for file in f:\n",
    "#         path = r+\"/\"+file\n",
    "#         print(path)\n",
    "#         if '.json' in file:\n",
    "#             file_ptr = open(path, \"r\")\n",
    "#             dic = json.load(file_ptr)\n",
    "#             for post in dic:\n",
    "#                 text = strip_punc(dic[post]['text_des'].lower())\n",
    "#                 toks = word_tokenize(text)\n",
    "#                 toks_ = []\n",
    "#                 for tok in toks:\n",
    "#                     tok = segment(tok)\n",
    "#                     for t in tok:\n",
    "#                         if not wordnet.synsets(t) or len(t) < 3:\n",
    "#                             continue\n",
    "\n",
    "#                         t = ps.stem(t)\n",
    "#                         if t not in stop_words:\n",
    "#                             toks_.append(t)\n",
    "\n",
    "#                 if toks_:\n",
    "#                     for topic in top100.keys():\n",
    "#                         if topic in path:\n",
    "#                             glob_corpora[topic].append(toks_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['architecture', 'nature'])\n",
      "architecture 4507\n",
      "nature 5020\n"
     ]
    }
   ],
   "source": [
    "print(glob_corpora.keys())\n",
    "for topic in top100.keys():\n",
    "    print(topic,len(glob_corpora[topic]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using word2vec for training\n",
    "glob_model = {}\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(\"./glove.6B/glove.6B.300d.w2vformat.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vatsal/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vatsal/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for topic in top100.keys():\n",
    "    print(topic)\n",
    "    glob_model[topic] = Word2Vec(glob_corpora[topic], size = 300, min_count = 2)\n",
    "#     glob_model[topic].build_vocab(glob_corpora[topic])\n",
    "    total_examples = glob_model[topic].corpus_count\n",
    "    glob_model[topic].build_vocab([list(model.vocab.keys())], update=True)\n",
    "    glob_model[topic].intersect_word2vec_format(\"./glove.6B/glove.6B.300d.w2vformat.txt\", binary=False, lockf=1.0)\n",
    "    glob_model[topic].train(glob_corpora[topic], total_examples=total_examples, epochs= glob_model[topic].iter)\n",
    "    glob_model[topic].save(\"./models/word2vec.\"+topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gensim.models.Word2Vec.load(\"./models/modelName.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(glob_model.wv.most_similar(positive = 'grass', topn = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 30 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Performing 30 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n"
     ]
    }
   ],
   "source": [
    "# Using Glove for training\n",
    "glove = {}\n",
    "\n",
    "for topic in top100.keys():\n",
    "    corpus.fit(glob_corpora[topic], window = 10)\n",
    "    glove[topic] = Glove(no_components = 100, learning_rate = 0.05)\n",
    "    glove[topic].fit(corpus.matrix, epochs = 30, no_threads = 4, verbose = True)\n",
    "    glove[topic].add_dictionary(corpus.dictionary)\n",
    "    glove[topic].save('./models/glove.model.'+topic)\n",
    "    glove[topic].add_dictionary(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['architecture', 'nature'])\n"
     ]
    }
   ],
   "source": [
    "print(top100.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['straight', 'sound', 'music', 'sceneri', 'wake', 'hike', 'mountain', 'bike', 'day', 'away', 'enjoy', 'role', 'hospit']\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing of input text\n",
    "\n",
    "# input_sentence = \"#forest#green#wanderlust#tour\"\n",
    "# input_sentence = \"Chocolate Entremet by Chef @karim.bourgi @beechefbyhansovando @hansovando @academia_fouet 🔥🔥🔥 #cakes #yummy #instagood #fan #tutorial #colourful #foodgasm #instacake #chocolate #chocolates #delicious #yummyyummy #food #cakestagram #amazing #instagram #pic #instayum #partage #socute #instagood #insta #instafood #follower #baking #instasweet #chocolate #foodstagram #pastrychef #instafood #instagram #pic #thafoodheaven #cakestagram #instayum #cakepops\"\n",
    "input_sentence = \"Straight out of The Sound of Music scenery is what you can wake up to, hiking and mountain biking your days away and enjoying Tyrolean hospitality.⁠\"\n",
    "text = strip_punc(input_sentence.lower())\n",
    "toks = word_tokenize(text)\n",
    "toks_ = []\n",
    "for tok in toks:\n",
    "    tok = segment(tok)\n",
    "    for t in tok:\n",
    "        if wordnet.synsets(t):\n",
    "            t = ps.stem(t)\n",
    "            if t not in stop_words and len(t)>2:\n",
    "                toks_.append(t)\n",
    "\n",
    "input_words = toks_\n",
    "print(input_words)\n",
    "\n",
    "topic = \"travel\"\n",
    "# topic = \"food\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'travel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-b40bb3dcdd65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'travel'"
     ]
    }
   ],
   "source": [
    "# Calculating sentence embedding using word2vec\n",
    "\n",
    "sent = []\n",
    "for word in input_words:\n",
    "    if word in glob_model[topic].wv.vocab:\n",
    "        sent.append(glob_model[topic].wv[word])\n",
    "    \n",
    "print(len(sent))\n",
    "sent = np.array(sent)\n",
    "print(sent.shape)\n",
    "embed = np.average(sent, axis = 0)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding similarity of top 100 hashtags using word2vec\n",
    "\n",
    "ans = []\n",
    "for hasht in top100_mapping[topic]:\n",
    "    try:\n",
    "        v1 = glob_model[topic].wv[hasht]\n",
    "        simi = np.dot(v1, embed) / (np.linalg.norm(v1) * np.linalg.norm(embed))\n",
    "        ans.append((simi, top100_mapping[topic][hasht]))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ans = sorted(ans, reverse = True)\n",
    "print(ans[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally top k for word2vec\n",
    "\n",
    "k = 10\n",
    "topk_word2vec = [h[1] for h in ans[:k]]\n",
    "print(topk_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating sentence embedding using glove\n",
    "\n",
    "sent = []\n",
    "print(input_words)\n",
    "for word in input_words:\n",
    "    if word in glove[topic].dictionary:\n",
    "        sent.append(glove[topic].word_vectors[glove[topic].dictionary[word]])\n",
    "    \n",
    "sent = np.array(sent)\n",
    "print(sent.shape)\n",
    "embed = np.average(sent, axis = 0)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(glove.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding similarity using glove embeddings\n",
    "\n",
    "ans2 = []\n",
    "for hasht in top100_mapping[topic]:\n",
    "    try:\n",
    "        v1 = glove[topic].word_vectors[glove[topic].dictionary[hasht]]\n",
    "        simi = np.dot(v1, embed) / (np.linalg.norm(v1) * np.linalg.norm(embed))\n",
    "        ans2.append((simi, top100_mapping[topic][hasht]))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ans2 = sorted(ans2, reverse = True)\n",
    "print(ans2[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally topk for glove\n",
    "k = 10\n",
    "\n",
    "topk_glove = [h[1] for h in ans2[:k]]\n",
    "print(topk_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Glove output\",topk_glove)\n",
    "print(\"Word2vec output\", topk_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
