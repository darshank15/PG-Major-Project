{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/vatsal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/vatsal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from nltk.corpus import wordnet\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer \n",
    "import gensim \n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entry(line): \n",
    "    w, c = line.split(\"\\t\", 2)\n",
    "    return (w, int(c))\n",
    "\n",
    "dict_path = \"./dict.txt\"\n",
    "dictionary = dict(entry(line) for line in open(dict_path))\n",
    "max_word_length = max(map(len, dictionary))\n",
    "total = float(sum(dictionary.values()))\n",
    "cleanup = re.compile(r'[^a-z0-9]')\n",
    "\n",
    "def word_prob(word): \n",
    "    return dictionary.get(word, 0) / total\n",
    "\n",
    "def segment(text): \n",
    "    text = re.sub(cleanup, ' ', text)\n",
    "    probs, lasts = [1.0], [0]\n",
    "    for i in range(1,len(text) + 1):\n",
    "        prob_k, k = max((probs[j] * word_prob(text[j:i]), j)for j in range(max(0, i - max_word_length), i))\n",
    "        probs.append(prob_k)\n",
    "        lasts.append(k)\n",
    "    words = []\n",
    "    i = len(text)\n",
    "    while i > 0:\n",
    "        words.append(text[lasts[i]:i])\n",
    "        i = lasts[i]\n",
    "    words.reverse()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Corpus, Glove\n",
    "corpus = Corpus() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_corpora = []\n",
    "top1000 = ['#travel', '#wanderlust', '#nature', '#travelling', '#traveling', '#traveller', '#photography', '#traveler', '#trip', '#travels', '#vacation', '#love', '#travelers', '#adventure', '#tourist', '#landscape', '#travellers', '#explore', '#holiday', '#beautiful', '#tourism', '#hiking', '#beach', '#photo', '#sunset', '#photographer', '#mountains', '#globetrotter', '#summer', '#art', '#sky', '#treking', '#europe', '#view', '#architecture', '#sea', '#happy', '#fun', '#city', '#sun', '#lifestyle', '#amazing', '#wanderer', '#italy', '#follow', '#backpacking', '#life', '#visiting', '#fashion', '#autumn', '#explorer', '#ocean', '#outdoors', '#india', '#world', '#mountain', '#beauty', '#spain', '#backpacker', '#style', '#like', '#france', '#exploring', '#trekking', '#clouds', '#asia', '#me', '#friends', '#usa', '#canon', '#happiness', '#blogger', '#holidays', '#ig', '#sunrise', '#smile', '#germany', '#girl', '#island', '#wander', '#paradise', '#turkey', '#discover', '#italia', '#voyage', '#flowers', '#landscapes', '#sightseeing', '#outdoor', '#history', '#indonesia', '#cute', '#forest', '#paris', '#food', '#australia', '#bali', '#pic', '#beaches', '#inspiration','#foodie', '#food', '#delicious', '#yummy', '#foodies', '#foods', '#eat', '#breakfast', '#dinner', '#tasty', '#cooking', '#lunch', '#homemade', '#dessert', '#love', '#eating', '#healthy', '#sweet', '#restaurant', '#photography', '#hungry', '#chef', '#blogger', '#travel', '#chocolate', '#baking', '#follow', '#cake', '#vegan', '#fresh', '#chicken', '#like', '#cook', '#amazing', '#blog', '#happy', '#favorite', '#brunch', '#coffee', '#weekend', '#fit', '#vegetarian', '#pasta', '#beautiful', '#pastry', '#fitness', '#gourmet', '#morning', '#desserts', '#seafood', '#eater', '#lifestyle', '#sweets', '#cafe', '#recipes', '#icecream', '#pizza', '#italy', '#culinary', '#candy', '#cheese', '#photographer', '#photo', '#cakes', '#recipe', '#noodles', '#diet', '#eats', '#rice', '#friends', '#cuisine', '#porridge', '#salad', '#nutrition', '#india', '#indonesia', '#bread', '#mornings', '#drinks', '#art', '#life', '#fun', '#gastronomy', '#sunday', '#cookies', '#kitchen', '#gym', '#me', '#beef', '#sushi', '#cupcakes', '#bake', '#spicy', '#saturday', '#fish', '#catering', '#burger', '#snack', '#music', '#delhi','#baby', '#kids', '#love', '#babies', '#cute', '#family', '#mom', '#fashion', '#newborn', '#children', '#happy', '#beautiful', '#photography', '#girl', '#babys', '#adorable', '#sweet', '#motherhood', '#pregnancy', '#handmade', '#funny', '#lovely', '#smile', '#mommy', '#life', '#daughter', '#style', '#follow', '#sweetheart', '#like', '#pregnant', '#beauty', '#precious', '#kid', '#child', '#fun', '#girls', '#mummy', '#tiny', '#twins', '#amazing', '#toddler', '#model', '#photo', '#mama', '#toys', '#boy', '#sleep', '#matching', '#autumn', '#enjoy', '#friends', '#pretty', '#art', '#parenting', '#live', '#mother', '#me', '#amor', '#photographer', '#princess', '#fairy', '#sale', '#happiness', '#boys', '#boutique', '#lifestyle', '#portrait', '#fall', '#son', '#pink', '#nature', '#childhood', '#cuteness', '#maternity', '#cool', '#travel', '#angel', '#best', '#halloween', '#play', '#weekend', '#october', '#dad', '#innocent', '#brand', '#makeup', '#home', '#parenthood', '#nice', '#little', '#sunday', '#flowers', '#mum', '#canon', '#infant', '#party', '#wedding', '#summer', '#hot','#jewellery', '#jewelry', '#fashion', '#accessories', '#earrings', '#necklace', '#style', '#jewels', '#handmade', '#beautiful', '#gemstone', '#trendy', '#gold', '#jewel', '#bracelet', '#rings', '#love', '#gems', '#silver', '#bracelets', '#design', '#ring', '#gemstones', '#crystals', '#shopping', '#stylish', '#art', '#pendant', '#diamond', '#cute', '#luxury', '#necklaces', '#bling', '#india', '#watches', '#handcrafted', '#designer', '#bijoux', '#trending', '#earring', '#bangles', '#bride', '#wedding', '#beauty', '#stone', '#tourmaline', '#charm', '#traditional', '#hyderabad', '#wholesaler', '#diamonds', '#bridal', '#photography', '#accessory', '#cabochons', '#tourmalines', '#stones', '#gem', '#goldplated', '#semiprecious', '#mumbai', '#silversmith', '#pendents', '#k', '#indian', '#sapphire', '#chennai', '#jewellers', '#girls', '#sale', '#jeweller', '#delhi', '#follow', '#fashionable', '#gift', '#bangalore', '#women', '#london', '#unique', '#ethnic', '#rawalpindi', '#crystal', '#vintage', '#oxidised', '#emerald', '#dubai', '#tucson', '#peridot', '#beads', '#makeup', '#saree', '#kunzite', '#chic', '#firecracker', '#pearl', '#lahore', '#choker', '#islamabad', '#wholesale', '#combo','#pet', '#pets', '#dog', '#cute', '#animals', '#love', '#dogs', '#cat', '#puppy', '#animal', '#cats', '#kitten', '#nature', '#kitty', '#meow', '#adorable', '#puppies', '#kittens', '#doggy', '#doggo', '#pup', '#photography', '#happy', '#funny', '#hound', '#eyes', '#beautiful', '#bunny', '#poodle', '#chihuahua', '#follow', '#rabbit', '#fluffy', '#paws', '#family', '#photo', '#lovely', '#chat', '#furry', '#daily', '#pomeranian', '#labrador', '#fun', '#autumn', '#sweet', '#maltese', '#smile', '#life', '#art', '#bunnies', '#hamster', '#baby', '#bird', '#me', '#world', '#bulldog', '#like', '#sleeping', '#tuesday', '#summer', '#parrots', '#husky', '#amor', '#rabbits', '#tot', '#zoo', '#doggie', '#meme', '#pug', '#istanbul', '#wildlife', '#beauty', '#adopt', '#girl', '#meowed', '#birds', '#purr', '#rescue', '#cavy', '#japan', '#doggies', '#halloween', '#photographer', '#woof', '#playtime', '#nice', '#portrait', '#relax', '#parakeet', '#singapore', '#parrot', '#fashion', '#fall', '#chihuahuas', '#friends', '#travel', '#morning', '#wolf', '#sweden', '#monday','#art', '#artist', '#drawing', '#artwork', '#illustration', '#sketch', '#painting', '#draw', '#sketchbook', '#creative', '#design', '#love', '#photography', '#beautiful', '#pencil', '#ink', '#illustrator', '#drawings', '#portrait', '#arts', '#gallery', '#artistic', '#fashion', '#paint', '#doodle', '#nature', '#picture', '#watercolor', '#graphic', '#color', '#photo', '#sketching', '#music', '#style', '#pen', '#anime', '#cute', '#paintings', '#follow', '#paper', '#graphics', '#artists', '#like', '#travel', '#abstract', '#tattoo', '#cartoon', '#girl', '#masterpiece', '#sketches', '#beauty', '#inspiration', '#procreate', '#designer', '#comics', '#artworks', '#photographer', '#life', '#digital', '#illustrations', '#model', '#handmade', '#watercolour', '#draws', '#happy', '#painter', '#black', '#colorful', '#colors', '#comic', '#fun', '#landscape', '#exhibition', '#cool', '#colour', '#architecture', '#aesthetic', '#acrylic', '#smile', '#graffiti', '#arty', '#tattoos', '#amazing', '#lifestyle', '#sculpture', '#blue', '#rap', '#halloween', '#inked', '#funny', '#fantasy', '#character', '#me', '#summer', '#singer', '#studio', '#logo', '#collage', '#doodles', '#pictures','#architecture', '#design', '#art', '#photography', '#building', '#travel', '#architect', '#city', '#buildings', '#urban', '#beautiful', '#style', '#minimal', '#street', '#architectural', '#interior', '#skyscraper', '#perspective', '#abstract', '#lines', '#town', '#architectures', '#geometric', '#designer', '#composition', '#geometry', '#arts', '#cities', '#home', '#landscape', '#nature', '#photo', '#photographer', '#interiors', '#house', '#italy', '#love', '#cityscape', '#modern', '#decor', '#pattern', '#luxury', '#sky', '#construction', '#architects', '#inspiration', '#arch', '#traveling', '#minimalism', '#render', '#wanderlust', '#sketch', '#rendering', '#europe', '#lifestyle', '#history', '#autumn', '#view', '#decoration', '#light', '#trip', '#amazing', '#concrete', '#paris', '#sunset', '#facade', '#artist', '#modernism', '#contemporary', '#sun', '#london', '#travelling', '#furniture', '#ig', '#life', '#france', '#church', '#traveller', '#canon', '#pic', '#explore', '#picture', '#details', '#colors', '#traveler', '#vacation', '#tourism', '#italia', '#milano', '#clouds', '#engineering', '#fashion', '#germany', '#spain', '#villa', '#wood', '#3d', '#exterior', '#blue', '#artwork','#nature', '#photography', '#landscape', '#travel', '#beautiful', '#love', '#autumn', '#photo', '#sky', '#photographer', '#mountains', '#sunset', '#outdoors', '#clouds', '#art', '#flowers', '#beauty', '#tree', '#wanderlust', '#follow', '#amazing', '#adventure', '#trees', '#forest', '#view', '#explore', '#green', '#summer', '#life', '#sun', '#hiking', '#colors', '#wildlife', '#mountain', '#travelling', '#fashion', '#like', '#canon', '#trip', '#sea', '#ig', '#happy', '#water', '#natural', '#fall', '#outdoor', '#beach', '#style', '#traveling', '#fun', '#blue', '#landscapes', '#sunrise', '#flower', '#animals', '#lake', '#cute', '#picture', '#macro', '#me', '#river', '#earth', '#natures', '#friends', '#leaves', '#followers', '#vacation', '#plants', '#weather', '#smile', '#garden', '#india', '#girl', '#pic', '#november', '#photos', '#world', '#wild', '#birds', '#model', '#traveller', '#peace', '#lifestyle', '#bird', '#walk', '#animal', '#holiday', '#discover', '#trekking', '#portrait', '#morning', '#nice', '#sunday', '#ocean', '#sunsets', '#traveler', '#fitness', '#rain', '#italy', '#winter']\n",
    "top1000 = set(top1000)\n",
    "top1000 = [r.replace('#', '') for r in top1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1000_stemmed = [ps.stem(word) for word in top1000]\n",
    "top1000_mapping = {}\n",
    "for i in range(0,100):\n",
    "    top1000_mapping[top1000_stemmed[i]]=top1000[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punc(s):\n",
    "    new_str = \"\"\n",
    "    for c in s:\n",
    "        if c in punctuation:\n",
    "            new_str += \" \"\n",
    "        else:\n",
    "            new_str += c\n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture\n",
      "./jsons/architecture/modernarchitecture.json\n",
      "./jsons/architecture/architecture.json\n",
      "./jsons/architecture/architecturelovers.json\n",
      "./jsons/architecture/architecturephotography.json\n",
      "./jsons/architecture/architecture_hunter.json\n",
      "./jsons/architecture/architectureporn.json\n",
      "5331\n",
      "art\n",
      "./jsons/art/artist.json\n",
      "./jsons/art/artistsoninstagram.json\n",
      "./jsons/art/art.json\n",
      "./jsons/art/artoftheday.json\n",
      "./jsons/art/drawing.json\n",
      "./jsons/art/artwork.json\n",
      "5317\n",
      "baby\n",
      "./jsons/baby/baby.json\n",
      "./jsons/baby/babyfashion.json\n",
      "./jsons/baby/babycute.json\n",
      "./jsons/baby/kids.json\n",
      "./jsons/baby/babylove.json\n",
      "./jsons/baby/babiesworld.json\n",
      "4179\n",
      "pet\n",
      "./jsons/pet/petstagram.json\n",
      "./jsons/pet/instapet.json\n",
      "./jsons/pet/petlover.json\n",
      "./jsons/pet/petscorner.json\n",
      "./jsons/pet/petsofinstagram.json\n",
      "./jsons/pet/pet.json\n",
      "5430\n",
      "food\n",
      "./jsons/food/foodphotography.json\n",
      "./jsons/food/foodporn.json\n",
      "./jsons/food/foodstyling.json\n",
      "./jsons/food/foodstagram.json\n",
      "./jsons/food/foodblog.json\n",
      "./jsons/food/foodaddict.json\n",
      "5450\n",
      "travel\n",
      "./jsons/travel/Luxarytravel.json\n",
      "./jsons/travel/travelbook.json\n",
      "./jsons/travel/Traveldeeper.json\n",
      "./jsons/travel/Hiking.json\n",
      "./jsons/travel/travelquotes.json\n",
      "./jsons/travel/travelstoke.json\n",
      "./jsons/travel/travel.json\n",
      "./jsons/travel/traveladdict.json\n",
      "./jsons/travel/travellersnotebook.json\n",
      "./jsons/travel/travelguide.json\n",
      "./jsons/travel/Travelabout.json\n",
      "./jsons/travel/trip.json\n",
      "./jsons/travel/solotravel.json\n",
      "./jsons/travel/Travelphotography.json\n",
      "./jsons/travel/travelbug.json\n",
      "./jsons/travel/travelpic.json\n",
      "./jsons/travel/travelgram.json\n",
      "./jsons/travel/tourist.json\n",
      "./jsons/travel/travelislife.json\n",
      "./jsons/travel/beachvibes.json\n",
      "./jsons/travel/treking.json\n",
      "./jsons/travel/Travelawesome.json\n",
      "./jsons/travel/traveltheglobe.json\n",
      "./jsons/travel/travelworld.json\n",
      "./jsons/travel/worldtraveller.json\n",
      "./jsons/travel/Travellove.json\n",
      "./jsons/travel/nature.json\n",
      "./jsons/travel/travelcaptures.json\n",
      "./jsons/travel/Citytravel.json\n",
      "24900\n",
      "nature\n",
      "./jsons/nature/nature_good.json\n",
      "./jsons/nature/naturelovers.json\n",
      "./jsons/nature/natureshot.json\n",
      "./jsons/nature/nature.json\n",
      "./jsons/nature/naturephotography.json\n",
      "./jsons/nature/naturebeauty.json\n",
      "5976\n",
      "jewellery\n",
      "./jsons/jewellery/jewelerydesign.json\n",
      "./jsons/jewellery/imitationjewellery.json\n",
      "./jsons/jewellery/fashionjewelry.json\n",
      "./jsons/jewellery/jewellery.json\n",
      "3853\n"
     ]
    }
   ],
   "source": [
    "for topic in [\"architecture\",\"art\",\"baby\",\"pet\",\"food\",\"travel\",\"nature\",\"jewellery\"]:\n",
    "    for r, d, f in os.walk(\"./jsons/\"+topic):\n",
    "        print(topic)\n",
    "        df = pd.read_csv('./csv/train/final_train_'+topic+\".csv\")\n",
    "        dic = dict()\n",
    "        for file in f:\n",
    "            path = r+\"/\"+file\n",
    "            print(path)\n",
    "            if '.json' in file:\n",
    "                file_ptr = open(path, \"r\")\n",
    "                tmp_dic = json.load(file_ptr)\n",
    "                dic.update(tmp_dic)\n",
    "                \n",
    "        print(len(dic))\n",
    "        for index,row in df.iterrows():\n",
    "            p_id = row['post_id'].split('.')[0]\n",
    "            try:\n",
    "                text = strip_punc(dic[p_id]['text_des'].lower())\n",
    "            except:\n",
    "                continue\n",
    "            toks = word_tokenize(text)\n",
    "            toks_ = []\n",
    "            for tok in toks:\n",
    "                tok = segment(tok)\n",
    "                for t in tok:\n",
    "                    if not wordnet.synsets(t) or len(t) < 3:\n",
    "                        continue\n",
    "\n",
    "                    t = ps.stem(t)\n",
    "                    if t not in stop_words:\n",
    "                        toks_.append(t)\n",
    "\n",
    "            if toks_:\n",
    "                glob_corpora.append(toks_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32860\n"
     ]
    }
   ],
   "source": [
    "print(len(glob_corpora))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vatsal/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Using word2vec for training\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(\"./glove.6B/glove.6B.300d.w2vformat.txt\", binary=False)\n",
    "\n",
    "glob_model = Word2Vec(glob_corpora, size=300, min_count = 2)\n",
    "total_examples = glob_model.corpus_count\n",
    "glob_model.build_vocab([list(model.vocab.keys())], update=True)\n",
    "glob_model.intersect_word2vec_format(\"./glove.6B/glove.6B.300d.w2vformat.txt\", binary=False, lockf=1.0)\n",
    "glob_model.train(glob_corpora, total_examples=total_examples, epochs= glob_model.iter)\n",
    "glob_model.save(\"./models/word2vec.global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pasta', 0.5810865163803101), ('sandwich', 0.5527045130729675), ('burger', 0.5419888496398926), ('taco', 0.520442008972168), ('pepperoni', 0.49301639199256897), ('steak', 0.4679437279701233), ('sushi', 0.4662565588951111), ('snack', 0.4659058451652527), ('gourmet', 0.4657880961894989), ('cheeseburg', 0.45455536246299744), ('mayonnais', 0.45111796259880066), ('bread', 0.4499778747558594), ('deli', 0.44754859805107117), ('grill', 0.44486141204833984), ('spaghetti', 0.43855801224708557), ('pizzeria', 0.4321829378604889), ('chicken', 0.43061619997024536), ('hut', 0.42878520488739014), ('diner', 0.4127577543258667), ('chees', 0.3979251980781555)]\n"
     ]
    }
   ],
   "source": [
    "print(glob_model.wv.most_similar(positive = 'pizza', topn = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 30 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n"
     ]
    }
   ],
   "source": [
    "# Using Glove for training\n",
    "\n",
    "corpus.fit(glob_corpora, window = 10)\n",
    "glove = Glove(no_components = 100, learning_rate = 0.05)\n",
    "glove.fit(corpus.matrix, epochs = 30, no_threads = 4, verbose = True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "glove.save('./models/glove.model.global')\n",
    "glove.add_dictionary(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing of input text\n",
    "\n",
    "input_sentence = \"#food#foodie\"\n",
    "input_sentence = \"A great week with many hours in the kitchen #madlejr #food #visitfyn #visitdenmark #kitchen #lovefood\"\n",
    "text = strip_punc(input_sentence.lower())\n",
    "toks = word_tokenize(text)\n",
    "toks_ = []\n",
    "for tok in toks:\n",
    "    tok = segment(tok)\n",
    "    for t in tok:\n",
    "        if wordnet.synsets(t):\n",
    "            t = ps.stem(t)\n",
    "            if t not in stop_words and len(t)>2:\n",
    "                toks_.append(t)\n",
    "\n",
    "input_words = toks_\n",
    "print(input_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating sentence embedding using word2vec\n",
    "\n",
    "sent = []\n",
    "for word in input_words:\n",
    "    if word in glob_model.wv.vocab:\n",
    "        sent.append(glob_model.wv[word])\n",
    "    \n",
    "print(len(sent))\n",
    "sent = np.array(sent)\n",
    "print(sent.shape)\n",
    "embed = np.average(sent, axis = 0)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding similarity of top 100 hashtags using word2vec\n",
    "\n",
    "ans = []\n",
    "for hasht in top1000_mapping:\n",
    "    try:\n",
    "        v1 = glob_model.wv[hasht]\n",
    "        simi = np.dot(v1, embed) / (np.linalg.norm(v1) * np.linalg.norm(embed))\n",
    "        ans.append((simi, top1000_mapping[hasht]))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ans = sorted(ans, reverse = True)\n",
    "print(ans[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally top k for word2vec\n",
    "\n",
    "k = 10\n",
    "topk_word2vec = [h[1] for h in ans[:k]]\n",
    "print(topk_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating sentence embedding using glove\n",
    "\n",
    "sent = []\n",
    "print(input_words)\n",
    "for word in input_words:\n",
    "    if word in glove.dictionary:\n",
    "        sent.append(glove.word_vectors[glove.dictionary[word]])\n",
    "    \n",
    "sent = np.array(sent)\n",
    "print(sent.shape)\n",
    "embed = np.average(sent, axis = 0)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding similarity using glove embeddings\n",
    "\n",
    "ans2 = []\n",
    "for hasht in top1000_mapping:\n",
    "    try:\n",
    "        v1 = glove.word_vectors[glove.dictionary[hasht]]\n",
    "        simi = np.dot(v1, embed) / (np.linalg.norm(v1) * np.linalg.norm(embed))\n",
    "        ans2.append((simi, top1000_mapping[hasht]))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ans2 = sorted(ans2, reverse = True)\n",
    "print(ans2[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally topk for glove\n",
    "k = 10\n",
    "\n",
    "topk_glove = [h[1] for h in ans2[:k]]\n",
    "print(topk_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Glove ouput\",topk_glove)\n",
    "print(\"word2vec output\", topk_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
