{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/vatsal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/vatsal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from nltk.corpus import wordnet\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer \n",
    "import gensim \n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import os\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Corpus, Glove\n",
    "corpus = Corpus() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_corpora = []\n",
    "top100 = ['#travel', '#wanderlust', '#nature', '#travelling', '#traveling', '#traveller', '#photography', '#traveler', '#trip', '#travels', '#vacation', '#love', '#travelers', '#adventure', '#tourist', '#landscape', '#travellers', '#holiday', '#explore', '#beautiful', '#tourism', '#hiking', '#beach', '#photo', '#sunset', '#photographer', '#mountains', '#globetrotter', '#summer', '#art', '#sky', '#treking', '#europe', '#view', '#architecture', '#sea', '#fun', '#happy', '#city', '#sun', '#amazing', '#lifestyle', '#backpacking', '#wanderer', '#italy', '#follow', '#life', '#visiting', '#fashion', '#autumn', '#ocean', '#outdoors', '#explorer', '#world', '#india', '#beauty', '#mountain', '#spain', '#style', '#backpacker', '#like', '#clouds', '#france', '#exploring', '#trekking', '#asia', '#me', '#friends', '#canon', '#usa', '#blogger', '#ig', '#happiness', '#sunrise', '#smile', '#holidays', '#girl', '#wander', '#germany', '#island', '#paradise', '#Travel', '#flowers', '#discover', '#voyage', '#turkey', '#sightseeing', '#landscapes', '#italia', '#outdoor', '#cute', '#indonesia', '#history', '#food', '#pic', '#forest', '#beaches', '#inspiration', '#green', '#memories']\n",
    "top100 = [r.replace('#', '') for r in top100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_stemmed = [ps.stem(word) for word in top100]\n",
    "top100_mapping = {}\n",
    "for i in range(0,100):\n",
    "    top100_mapping[top100_stemmed[i]]=top100[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punc(s):\n",
    "    new_str = \"\"\n",
    "    for c in s:\n",
    "        if c in punctuation:\n",
    "            new_str += \" \"\n",
    "        else:\n",
    "            new_str += c\n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./jsons/Luxarytravel.json\n",
      "./jsons/travelbook.json\n",
      "./jsons/Traveldeeper.json\n",
      "./jsons/Hiking.json\n",
      "./jsons/travelquotes.json\n",
      "./jsons/travelstoke.json\n",
      "./jsons/travel.json\n",
      "./jsons/traveladdict.json\n",
      "./jsons/travellersnotebook.json\n",
      "./jsons/travelguide.json\n",
      "./jsons/Travelabout.json\n",
      "./jsons/trip.json\n",
      "./jsons/solotravel.json\n",
      "./jsons/Travelphotography.json\n",
      "./jsons/travelbug.json\n",
      "./jsons/travelpic.json\n",
      "./jsons/travelgram.json\n",
      "./jsons/tourist.json\n",
      "./jsons/travelislife.json\n",
      "./jsons/beachvibes.json\n",
      "./jsons/treking.json\n",
      "./jsons/Travelawesome.json\n",
      "./jsons/traveltheglobe.json\n",
      "./jsons/travelworld.json\n",
      "./jsons/worldtraveller.json\n",
      "./jsons/Travellove.json\n",
      "./jsons/nature.json\n",
      "./jsons/travelcaptures.json\n",
      "./jsons/Citytravel.json\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"./jsons/\"):\n",
    "    print(\"./jsons/\" + file)\n",
    "    file_ptr = open(\"./jsons/\" + file, \"r\")\n",
    "    dic = json.load(file_ptr)\n",
    "    for post in dic:\n",
    "        text = strip_punc(dic[post]['text_des'].lower())\n",
    "        toks = word_tokenize(text)\n",
    "        toks_ = []\n",
    "        for tok in toks:\n",
    "            if not wordnet.synsets(tok) or len(tok) < 3:\n",
    "                continue\n",
    "            tok = ps.stem(tok)\n",
    "            if tok not in stop_words:\n",
    "                toks_.append(tok)\n",
    "        if toks_:\n",
    "            glob_corpora.append(toks_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25481\n"
     ]
    }
   ],
   "source": [
    "print(len(glob_corpora))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using word2vec for training\n",
    "\n",
    "glob_model = Word2Vec(glob_corpora, min_count = 2, size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mist', 0.9922024011611938), ('bamboo', 0.990009069442749), ('foggi', 0.9859453439712524), ('dawn', 0.9804486036300659), ('fog', 0.9797745943069458), ('wood', 0.9797183275222778), ('pomegran', 0.9796435832977295), ('lush', 0.9788281321525574), ('clad', 0.9786802530288696), ('kazakh', 0.9773837924003601), ('wasteland', 0.9763006567955017), ('woodland', 0.9761473536491394), ('hoop', 0.9753665924072266), ('hors', 0.9752497673034668), ('leaf', 0.9750267863273621), ('rainbow', 0.9740670919418335), ('inconceiv', 0.9730759859085083), ('syrup', 0.9726815819740295), ('mane', 0.9717535376548767), ('montana', 0.9717276096343994)]\n"
     ]
    }
   ],
   "source": [
    "print(glob_model.wv.most_similar(positive = 'grass', topn = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 30 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n"
     ]
    }
   ],
   "source": [
    "# Using Glove for training\n",
    "\n",
    "corpus.fit(glob_corpora, window = 10)\n",
    "glove = Glove(no_components = 300, learning_rate = 0.05)\n",
    "glove.fit(corpus.matrix, epochs = 30, no_threads = 4, verbose = True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "glove.save('glove.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['citi', 'life', 'enjoy', 'metro', 'car']\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing of input text\n",
    "\n",
    "input_sentence = \"city life enjoyment metro car\"\n",
    "text = strip_punc(input_sentence.lower())\n",
    "toks = word_tokenize(text)\n",
    "toks_ = []\n",
    "for tok in toks:\n",
    "    if wordnet.synsets(tok):\n",
    "        tok = ps.stem(tok)\n",
    "        if tok not in stop_words and len(tok)>2:\n",
    "            toks_.append(tok)\n",
    "\n",
    "input_words = toks_\n",
    "print(input_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(5, 300)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "# Calculating sentence embedding using word2vec\n",
    "\n",
    "sent = []\n",
    "for word in input_words:\n",
    "    if word in glob_model.wv.vocab:\n",
    "        sent.append(glob_model.wv[word])\n",
    "    \n",
    "print(len(sent))\n",
    "sent = np.array(sent)\n",
    "print(sent.shape)\n",
    "embed = np.average(sent, axis = 0)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.8402681, 'life'), (0.83269423, 'memories'), (0.76508474, 'cute'), (0.7484694, 'smile'), (0.7464194, 'ig'), (0.7383746, 'beauty'), (0.7346288, 'inspiration'), (0.73444617, 'love'), (0.70989937, 'happiness'), (0.6989758, 'girl'), (0.69524664, 'fun'), (0.6729203, 'like'), (0.66195184, 'style'), (0.6618949, 'amazing'), (0.6576407, 'friends'), (0.6469303, 'city'), (0.64483446, 'food'), (0.62304217, 'sightseeing'), (0.60987926, 'fashion'), (0.6062473, 'summer')]\n"
     ]
    }
   ],
   "source": [
    "# Finding similarity of top 100 hashtags using word2vec\n",
    "\n",
    "ans = []\n",
    "for hasht in top100_mapping:\n",
    "    try:\n",
    "        v1 = glob_model.wv[hasht]\n",
    "        simi = np.dot(v1, embed) / (np.linalg.norm(v1) * np.linalg.norm(embed))\n",
    "        ans.append((simi, top100_mapping[hasht]))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ans = sorted(ans, reverse = True)\n",
    "print(ans[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'memories', 'cute', 'smile', 'ig', 'beauty', 'inspiration', 'love', 'happiness', 'girl']\n"
     ]
    }
   ],
   "source": [
    "# Finally top k for word2vec\n",
    "\n",
    "k = 10\n",
    "topk_word2vec = [h[1] for h in ans[:k]]\n",
    "print(topk_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['citi', 'life', 'enjoy', 'metro', 'car']\n",
      "(5, 300)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "# Calculating sentence embedding using glove\n",
    "\n",
    "sent = []\n",
    "print(input_words)\n",
    "for word in input_words:\n",
    "    if word in glove.dictionary:\n",
    "        sent.append(glove.word_vectors[glove.dictionary[word]])\n",
    "    \n",
    "sent = np.array(sent)\n",
    "print(sent.shape)\n",
    "embed = np.average(sent, axis = 0)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7850062647418138, 'life'), (0.6989680952474028, 'city'), (0.6901631371832204, 'memories'), (0.6113891536112518, 'lifestyle'), (0.6095009959491364, 'trip'), (0.5776050934044664, 'exploring'), (0.5730859192811777, 'visiting'), (0.5718418631289597, 'beauty'), (0.5648133073923786, 'fun'), (0.5558073084679296, 'history'), (0.5515839359251011, 'amazing'), (0.5461834018353884, 'adventure'), (0.5426864478934118, 'photography'), (0.5347772889125532, 'inspiration'), (0.5319193704254476, 'smile'), (0.5239102348067891, 'love'), (0.5228078829499611, 'vacation'), (0.5195034870389083, 'happiness'), (0.5155602216077344, 'holidays'), (0.5149448796976712, 'friends')]\n"
     ]
    }
   ],
   "source": [
    "# Finding similarity using glove embeddings\n",
    "\n",
    "ans2 = []\n",
    "for hasht in top100_mapping:\n",
    "    try:\n",
    "        v1 = glove.word_vectors[glove.dictionary[hasht]]\n",
    "        simi = np.dot(v1, embed) / (np.linalg.norm(v1) * np.linalg.norm(embed))\n",
    "        ans2.append((simi, top100_mapping[hasht]))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ans2 = sorted(ans2, reverse = True)\n",
    "print(ans2[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'city', 'memories', 'lifestyle', 'trip', 'exploring', 'visiting', 'beauty', 'fun', 'history']\n"
     ]
    }
   ],
   "source": [
    "# Finally topk for glove\n",
    "\n",
    "k = 10\n",
    "\n",
    "topk_glove = [h[1] for h in ans2[:k]]\n",
    "print(topk_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'city', 'memories', 'lifestyle', 'trip', 'exploring', 'visiting', 'beauty', 'fun', 'history'] ['life', 'memories', 'cute', 'smile', 'ig', 'beauty', 'inspiration', 'love', 'happiness', 'girl']\n"
     ]
    }
   ],
   "source": [
    "print(topk_glove, topk_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25481\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './glove.6B/glove.6B.300d.w2vformat.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b577526633fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtotal_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./glove.6B/glove.6B.300d.w2vformat.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './glove.6B/glove.6B.300d.w2vformat.txt'"
     ]
    }
   ],
   "source": [
    "# Training on top of pre-trained 100-D word embeddings\n",
    "\n",
    "model_2 = Word2Vec(size = 300, min_count = 2)\n",
    "model_2.build_vocab(glob_corpora)\n",
    "total_examples = model_2.corpus_count\n",
    "model = KeyedVectors.load_word2vec_format(\"./glove.6B/glove.6B.300d.w2vformat.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1608376, 1942600)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.build_vocab([list(model.vocab.keys())], update=True)\n",
    "model_2.intersect_word2vec_format(\"./glove.6B/glove.6B.300d.w2vformat.txt\", binary=False, lockf=1.0)\n",
    "model_2.train(glob_corpora, total_examples=total_examples, epochs=model_2.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(5, 100)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# Calculating sentence embedding using word2vec built on top\n",
    "\n",
    "sent = []\n",
    "for word in input_words:\n",
    "    if word in model_2.wv.vocab:\n",
    "        sent.append(model_2.wv[word])\n",
    "    \n",
    "print(len(sent))\n",
    "sent = np.array(sent)\n",
    "print(sent.shape)\n",
    "embed = np.average(sent, axis = 0)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1590738238310768, 'life'), (0.1568674105766756, 'inspiration'), (0.14864744117986625, 'lifestyle'), (0.14443303061448606, 'green'), (0.1438923174633605, 'happiness'), (0.13621081185388623, 'architecture'), (0.12903651775479635, 'style'), (0.12686728826314567, 'smile'), (0.12567769092664607, 'beauty'), (0.12381021717285125, 'art'), (0.12063842743665494, 'autumn'), (0.1157567075987542, 'history'), (0.11364085842518182, 'spain'), (0.11207138762507324, 'love'), (0.11107323926624592, 'food'), (0.11090056420187287, 'fashion'), (0.1086669006549339, 'landscapes'), (0.09683028480330677, 'cute'), (0.08633663624716623, 'forest'), (0.08347055978678607, 'summer')]\n"
     ]
    }
   ],
   "source": [
    "# Finding similarity of top 100 hashtags using word2vec built on top\n",
    "\n",
    "ans3 = []\n",
    "for hasht in top100_mapping:\n",
    "    try:\n",
    "        v1 = model_2.wv[hasht]\n",
    "        simi = np.dot(v1, embed) / (np.linalg.norm(v1) * np.linalg.norm(embed))\n",
    "        ans3.append((simi, top100_mapping[hasht]))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ans3 = sorted(ans3, reverse = True)\n",
    "print(ans3[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'inspiration', 'lifestyle', 'green', 'happiness', 'architecture', 'style', 'smile', 'beauty', 'art'] ['life', 'memories', 'cute', 'smile', 'beauty', 'love', 'inspiration', 'ig', 'happiness', 'fun']\n"
     ]
    }
   ],
   "source": [
    "topk_word2vec_on_top = [h[1] for h in ans3[:k]] \n",
    "print(topk_word2vec_on_top, topk_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vatsal Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Glove for training\n",
    "\n",
    "corpus.fit(glob_corpora, window = 10)\n",
    "glove = Glove(no_components = 300, learning_rate = 0.05)\n",
    "glove.fit(corpus.matrix, epochs = 30, no_threads = 4, verbose = True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "glove.save('glove.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vatsal Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input  city life enjoyment metro car\n",
      "Glove output\n",
      "  ['life', 'city', 'memories', 'visiting', 'trip', 'lifestyle', 'fun', 'history', 'inspiration', 'exploring']\n",
      "Word2Vec output\n",
      "  ['life', 'memories', 'cute', 'smile', 'beauty', 'love', 'inspiration', 'ig', 'happiness', 'fun']\n",
      "Word2Vec output with training on top of pre-trained\n",
      "  ['life', 'inspiration', 'lifestyle', 'green', 'happiness', 'architecture', 'style', 'smile', 'beauty', 'art']\n"
     ]
    }
   ],
   "source": [
    "print(\"Input \", input_sentence)\n",
    "print(\"Glove output\\n \", topk_glove)\n",
    "print(\"Word2Vec output\\n \", topk_word2vec)\n",
    "print(\"Word2Vec output with training on top of pre-trained\\n \", topk_word2vec_on_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sequoia', 0.993638813495636), ('confetti', 0.9873069524765015), ('dawn', 0.9858814477920532), ('mist', 0.9847056865692139), ('wildflow', 0.984110414981842), ('hue', 0.9838014841079712), ('cowgirl', 0.9802432060241699), ('textur', 0.9802113175392151), ('windi', 0.9799920320510864), ('stream', 0.9789674282073975)]\n",
      "[('lawn', 0.5653282403945923), ('greeneri', 0.546645998954773), ('wildflow', 0.539504885673523), ('ferocactu', 0.5318676829338074), ('inflat', 0.5228531360626221), ('tourmalin', 0.5141727924346924), ('horticultur', 0.5055184960365295), ('agricultur', 0.5021069049835205), ('mojav', 0.5011772513389587), ('shorelin', 0.49985647201538086)]\n"
     ]
    }
   ],
   "source": [
    "test = \"fun\"\n",
    "# test = \"grass\"\n",
    "print(glob_model.wv.most_similar(positive = test, topn = 10))\n",
    "print(model_2.wv.most_similar(positive = test, topn = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = Word2Vec(size = 100, min_count = 1)\n",
    "temp.build_vocab(sentences)\n",
    "# print(temp.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2 = KeyedVectors.load_word2vec_format(\"./glove.6B/glove.6B.100d.w2vformat.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(count:399915, index:85)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(temp2.wv.vocab['world'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.build_vocab([list(temp2.vocab.keys())], update=True)\n",
    "# model_2.intersect_word2vec_format(\"./glove.6B/glove.6B.100d.w2vformat.txt\", binary=False, lockf=1.0)\n",
    "# model_2.train(glob_corpora, total_examples=total_examples, epochs=model_2.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400002\n"
     ]
    }
   ],
   "source": [
    "print(len(list(temp.wv.vocab.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8604\n"
     ]
    }
   ],
   "source": [
    "print(len(list(model_2.wv.vocab.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
