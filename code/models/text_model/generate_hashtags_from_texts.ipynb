{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/vatsal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/vatsal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from nltk.corpus import wordnet\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "import os\n",
    "import re\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entry(line): \n",
    "    w, c = line.split(\"\\t\", 2)\n",
    "    return (w, int(c))\n",
    "\n",
    "dict_path = \"./dict.txt\"\n",
    "dictionary = dict(entry(line) for line in open(dict_path))\n",
    "max_word_length = max(map(len, dictionary))\n",
    "total = float(sum(dictionary.values()))\n",
    "cleanup = re.compile(r'[^a-z0-9]')\n",
    "\n",
    "def word_prob(word): \n",
    "    return dictionary.get(word, 0) / total\n",
    "\n",
    "def segment(text): \n",
    "    text = re.sub(cleanup, ' ', text)\n",
    "    probs, lasts = [1.0], [0]\n",
    "    for i in range(1,len(text) + 1):\n",
    "        prob_k, k = max((probs[j] * word_prob(text[j:i]), j)for j in range(max(0, i - max_word_length), i))\n",
    "        probs.append(prob_k)\n",
    "        lasts.append(k)\n",
    "    words = []\n",
    "    i = len(text)\n",
    "    while i > 0:\n",
    "        words.append(text[lasts[i]:i])\n",
    "        i = lasts[i]\n",
    "    words.reverse()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Corpus, Glove\n",
    "corpus = Corpus() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_corpora = []\n",
    "top100 = ['#travel', '#wanderlust', '#nature', '#travelling', '#traveling', '#traveller', '#photography', '#traveler', '#trip', '#travels', '#vacation', '#love', '#travelers', '#adventure', '#tourist', '#landscape', '#travellers', '#holiday', '#explore', '#beautiful', '#tourism', '#hiking', '#beach', '#photo', '#sunset', '#photographer', '#mountains', '#globetrotter', '#summer', '#art', '#sky', '#treking', '#europe', '#view', '#architecture', '#sea', '#fun', '#happy', '#city', '#sun', '#amazing', '#lifestyle', '#backpacking', '#wanderer', '#italy', '#follow', '#life', '#visiting', '#fashion', '#autumn', '#ocean', '#outdoors', '#explorer', '#world', '#india', '#beauty', '#mountain', '#spain', '#style', '#backpacker', '#like', '#clouds', '#france', '#exploring', '#trekking', '#asia', '#me', '#friends', '#canon', '#usa', '#blogger', '#ig', '#happiness', '#sunrise', '#smile', '#holidays', '#girl', '#wander', '#germany', '#island', '#paradise', '#Travel', '#flowers', '#discover', '#voyage', '#turkey', '#sightseeing', '#landscapes', '#italia', '#outdoor', '#cute', '#indonesia', '#history', '#food', '#pic', '#forest', '#beaches', '#inspiration', '#green', '#memories']\n",
    "top100 = [r.replace('#', '') for r in top100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_stemmed = [ps.stem(word) for word in top100]\n",
    "top100_mapping = {}\n",
    "for i in range(0,100):\n",
    "    top100_mapping[top100_stemmed[i]]=top100[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punc(s):\n",
    "    new_str = \"\"\n",
    "    for c in s:\n",
    "        if c in punctuation:\n",
    "            new_str += \" \"\n",
    "        else:\n",
    "            new_str += c\n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./jsons/travel/Luxarytravel.json\n",
      "./jsons/travel/travelbook.json\n",
      "./jsons/travel/Traveldeeper.json\n",
      "./jsons/travel/Hiking.json\n",
      "./jsons/travel/travelquotes.json\n",
      "./jsons/travel/travelstoke.json\n",
      "./jsons/travel/travel.json\n",
      "./jsons/travel/traveladdict.json\n",
      "./jsons/travel/travellersnotebook.json\n",
      "./jsons/travel/travelguide.json\n",
      "./jsons/travel/Travelabout.json\n",
      "./jsons/travel/trip.json\n",
      "./jsons/travel/solotravel.json\n",
      "./jsons/travel/Travelphotography.json\n",
      "./jsons/travel/travelbug.json\n",
      "./jsons/travel/travelpic.json\n",
      "./jsons/travel/travelgram.json\n",
      "./jsons/travel/tourist.json\n",
      "./jsons/travel/travelislife.json\n",
      "./jsons/travel/beachvibes.json\n",
      "./jsons/travel/treking.json\n",
      "./jsons/travel/Travelawesome.json\n",
      "./jsons/travel/traveltheglobe.json\n",
      "./jsons/travel/travelworld.json\n",
      "./jsons/travel/worldtraveller.json\n",
      "./jsons/travel/Travellove.json\n",
      "./jsons/travel/nature.json\n",
      "./jsons/travel/travelcaptures.json\n",
      "./jsons/travel/Citytravel.json\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"./jsons/travel\"):\n",
    "    print(\"./jsons/travel/\" + file)\n",
    "    file_ptr = open(\"./jsons/travel/\" + file, \"r\")\n",
    "    dic = json.load(file_ptr)\n",
    "    for post in dic:\n",
    "        text = strip_punc(dic[post]['text_des'].lower())\n",
    "        toks = word_tokenize(text)\n",
    "        toks_ = []\n",
    "        for tok in toks:\n",
    "            tok = segment(tok)\n",
    "            for t in tok:\n",
    "                if not wordnet.synsets(t) or len(t) < 3:\n",
    "                    continue\n",
    "\n",
    "                t = ps.stem(t)\n",
    "                if t not in stop_words:\n",
    "                    toks_.append(t)\n",
    "            \n",
    "        if toks_:\n",
    "            glob_corpora.append(toks_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25764\n"
     ]
    }
   ],
   "source": [
    "print(len(glob_corpora))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using word2vec for training\n",
    "\n",
    "glob_model = Word2Vec(glob_corpora, min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pine', 0.9038450717926025), ('wood', 0.8921689987182617), ('poni', 0.8802863359451294), ('hydrangea', 0.8703752756118774), ('fog', 0.8680858612060547), ('leaf', 0.8678985238075256), ('meadow', 0.8659888505935669), ('foggi', 0.8658663034439087), ('terri', 0.8652238249778748), ('cynic', 0.8553472757339478), ('woodland', 0.8542168736457825), ('orchid', 0.8531167507171631), ('butterfli', 0.8529670238494873), ('shade', 0.8500607013702393), ('rainbow', 0.8481327295303345), ('downhil', 0.8466649055480957), ('majest', 0.8458655476570129), ('snowi', 0.8452242612838745), ('archeri', 0.845212996006012), ('mustang', 0.8440729975700378)]\n"
     ]
    }
   ],
   "source": [
    "print(glob_model.wv.most_similar(positive = 'grass', topn = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 30 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n"
     ]
    }
   ],
   "source": [
    "# Using Glove for training\n",
    "\n",
    "corpus.fit(glob_corpora, window = 10)\n",
    "glove = Glove(no_components = 100, learning_rate = 0.05)\n",
    "glove.fit(corpus.matrix, epochs = 30, no_threads = 4, verbose = True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "glove.save('glove.model')\n",
    "glove.add_dictionary(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['travel', 'trip', 'photo', 'pic', 'day', 'holiday', 'summer', 'fun', 'drone', 'moment', 'photographi', 'focu', 'pic', 'color', 'travel', 'happi', 'holiday', 'passport', 'travel', 'gram', 'photo', 'day', 'travel', 'travel', 'exposur', 'captur', 'snapshot', 'travel', 'photographi']\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing of input text\n",
    "\n",
    "# input_sentence = \"#forest#green#wanderlust#tour\"\n",
    "# input_sentence = \"Straight out of The Sound of Music scenery is what you can wake up to, hiking and mountain biking your days away and enjoying Tyrolean hospitality.â \n",
    "input_sentence = \"#Travel #trip #photo #picoftheday #holiday #summer2018# fun #drone #moment #photography #focus #pic #color #igtravel #happyholidays #instapassport #travelgram #tflers #photooftheday #instatravel #travelingram #exposure #capture #snapshot #traveling #instago #photographyislifee\"\n",
    "text = strip_punc(input_sentence.lower())\n",
    "toks = word_tokenize(text)\n",
    "toks_ = []\n",
    "for tok in toks:\n",
    "    tok = segment(tok)\n",
    "    for t in tok:\n",
    "        if wordnet.synsets(t):\n",
    "            t = ps.stem(t)\n",
    "            if t not in stop_words and len(t)>2:\n",
    "                toks_.append(t)\n",
    "\n",
    "input_words = toks_\n",
    "print(input_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "(13, 100)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# Calculating sentence embedding using word2vec\n",
    "\n",
    "sent = []\n",
    "for word in input_words:\n",
    "    if word in glob_model.wv.vocab:\n",
    "        sent.append(glob_model.wv[word])\n",
    "    \n",
    "print(len(sent))\n",
    "sent = np.array(sent)\n",
    "print(sent.shape)\n",
    "embed = np.average(sent, axis = 0)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.79706544, 'hiking'), (0.7241504, 'mountain'), (0.6522338, 'trekking'), (0.6347725, 'forest'), (0.61286086, 'green'), (0.590361, 'outdoor'), (0.5352306, 'clouds'), (0.52261513, 'adventure'), (0.48653263, 'happiness'), (0.46745187, 'landscapes'), (0.46624708, 'nature'), (0.45242748, 'ig'), (0.4503826, 'love'), (0.4496548, 'fun'), (0.44890246, 'sunrise'), (0.44407701, 'sky'), (0.4428573, 'view'), (0.42693922, 'exploring'), (0.42463803, 'backpacker'), (0.42436984, 'autumn')]\n"
     ]
    }
   ],
   "source": [
    "# Finding similarity of top 100 hashtags using word2vec\n",
    "\n",
    "ans = []\n",
    "for hasht in top100_mapping:\n",
    "    try:\n",
    "        v1 = glob_model.wv[hasht]\n",
    "        simi = np.dot(v1, embed) / (np.linalg.norm(v1) * np.linalg.norm(embed))\n",
    "        ans.append((simi, top100_mapping[hasht]))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ans = sorted(ans, reverse = True)\n",
    "print(ans[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hiking', 'mountain', 'trekking', 'forest', 'green', 'outdoor', 'clouds', 'adventure', 'happiness', 'landscapes']\n"
     ]
    }
   ],
   "source": [
    "# Finally top k for word2vec\n",
    "\n",
    "k = 10\n",
    "topk_word2vec = [h[1] for h in ans[:k]]\n",
    "print(topk_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['straight', 'sound', 'music', 'sceneri', 'wake', 'hike', 'mountain', 'bike', 'day', 'away', 'enjoy', 'role', 'hospit']\n",
      "(13, 100)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# Calculating sentence embedding using glove\n",
    "\n",
    "sent = []\n",
    "print(input_words)\n",
    "for word in input_words:\n",
    "    if word in glove.dictionary:\n",
    "        sent.append(glove.word_vectors[glove.dictionary[word]])\n",
    "    \n",
    "sent = np.array(sent)\n",
    "print(sent.shape)\n",
    "embed = np.average(sent, axis = 0)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7453365819867968, 'mountain'), (0.7298255248162238, 'hiking'), (0.6122607011664623, 'happiness'), (0.6106694595571494, 'life'), (0.5710631251832585, 'summer'), (0.5665188474463855, 'trekking'), (0.5631765135574418, 'love'), (0.5479993013866538, 'outdoor'), (0.5466948142020419, 'friends'), (0.5455864144595995, 'green'), (0.5374060591790857, 'beauty'), (0.5373822325422785, 'adventure'), (0.53635091556927, 'view'), (0.535265329724896, 'fun'), (0.5232745004195499, 'pic'), (0.5168948279325701, 'wanderlust'), (0.5093231041139552, 'nature'), (0.5072057520100826, 'memories'), (0.5071905281011232, 'clouds'), (0.5029636688800222, 'like')]\n"
     ]
    }
   ],
   "source": [
    "# Finding similarity using glove embeddings\n",
    "\n",
    "ans2 = []\n",
    "for hasht in top100_mapping:\n",
    "    try:\n",
    "        v1 = glove.word_vectors[glove.dictionary[hasht]]\n",
    "        simi = np.dot(v1, embed) / (np.linalg.norm(v1) * np.linalg.norm(embed))\n",
    "        ans2.append((simi, top100_mapping[hasht]))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ans2 = sorted(ans2, reverse = True)\n",
    "print(ans2[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mountain', 'hiking', 'happiness', 'life', 'summer', 'trekking', 'love', 'outdoor', 'friends', 'green']\n"
     ]
    }
   ],
   "source": [
    "# Finally topk for glove\n",
    "k = 10\n",
    "\n",
    "topk_glove = [h[1] for h in ans2[:k]]\n",
    "print(topk_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove output ['mountain', 'hiking', 'happiness', 'life', 'summer', 'trekking', 'love', 'outdoor', 'friends', 'green']\n",
      "Word2vec output ['hiking', 'mountain', 'trekking', 'forest', 'green', 'outdoor', 'clouds', 'adventure', 'happiness', 'landscapes']\n"
     ]
    }
   ],
   "source": [
    "print(\"Glove output\",topk_glove)\n",
    "print(\"Word2vec output\", topk_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
