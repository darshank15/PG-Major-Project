{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/vatsal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/vatsal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from nltk.corpus import wordnet\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer \n",
    "import gensim \n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import os\n",
    "import re\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entry(line): \n",
    "    w, c = line.split(\"\\t\", 2)\n",
    "    return (w, int(c))\n",
    "\n",
    "dict_path = \"./dict.txt\"\n",
    "dictionary = dict(entry(line) for line in open(dict_path))\n",
    "max_word_length = max(map(len, dictionary))\n",
    "total = float(sum(dictionary.values()))\n",
    "cleanup = re.compile(r'[^a-z0-9]')\n",
    "\n",
    "def word_prob(word): \n",
    "    return dictionary.get(word, 0) / total\n",
    "\n",
    "def segment(text): \n",
    "    text = re.sub(cleanup, ' ', text)\n",
    "    probs, lasts = [1.0], [0]\n",
    "    for i in range(1,len(text) + 1):\n",
    "        prob_k, k = max((probs[j] * word_prob(text[j:i]), j)for j in range(max(0, i - max_word_length), i))\n",
    "        probs.append(prob_k)\n",
    "        lasts.append(k)\n",
    "    words = []\n",
    "    i = len(text)\n",
    "    while i > 0:\n",
    "        words.append(text[lasts[i]:i])\n",
    "        i = lasts[i]\n",
    "    words.reverse()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Corpus, Glove\n",
    "corpus = Corpus() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_corpora = []\n",
    "top100 = ['#travel', '#wanderlust', '#nature', '#travelling', '#traveling', '#traveller', '#photography', '#traveler', '#trip', '#travels', '#vacation', '#love', '#travelers', '#adventure', '#tourist', '#landscape', '#travellers', '#holiday', '#explore', '#beautiful', '#tourism', '#hiking', '#beach', '#photo', '#sunset', '#photographer', '#mountains', '#globetrotter', '#summer', '#art', '#sky', '#treking', '#europe', '#view', '#architecture', '#sea', '#fun', '#happy', '#city', '#sun', '#amazing', '#lifestyle', '#backpacking', '#wanderer', '#italy', '#follow', '#life', '#visiting', '#fashion', '#autumn', '#ocean', '#outdoors', '#explorer', '#world', '#india', '#beauty', '#mountain', '#spain', '#style', '#backpacker', '#like', '#clouds', '#france', '#exploring', '#trekking', '#asia', '#me', '#friends', '#canon', '#usa', '#blogger', '#ig', '#happiness', '#sunrise', '#smile', '#holidays', '#girl', '#wander', '#germany', '#island', '#paradise', '#Travel', '#flowers', '#discover', '#voyage', '#turkey', '#sightseeing', '#landscapes', '#italia', '#outdoor', '#cute', '#indonesia', '#history', '#food', '#pic', '#forest', '#beaches', '#inspiration', '#green', '#memories']\n",
    "top100 = [r.replace('#', '') for r in top100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_stemmed = [ps.stem(word) for word in top100]\n",
    "top100_mapping = {}\n",
    "for i in range(0,100):\n",
    "    top100_mapping[top100_stemmed[i]]=top100[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punc(s):\n",
    "    new_str = \"\"\n",
    "    for c in s:\n",
    "        if c in punctuation:\n",
    "            new_str += \" \"\n",
    "        else:\n",
    "            new_str += c\n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./jsons/travel/Luxarytravel.json\n",
      "./jsons/travel/travelbook.json\n",
      "./jsons/travel/Traveldeeper.json\n",
      "./jsons/travel/Hiking.json\n",
      "./jsons/travel/travelquotes.json\n",
      "./jsons/travel/travelstoke.json\n",
      "./jsons/travel/travel.json\n",
      "./jsons/travel/traveladdict.json\n",
      "./jsons/travel/travellersnotebook.json\n",
      "./jsons/travel/travelguide.json\n",
      "./jsons/travel/Travelabout.json\n",
      "./jsons/travel/trip.json\n",
      "./jsons/travel/solotravel.json\n",
      "./jsons/travel/Travelphotography.json\n",
      "./jsons/travel/travelbug.json\n",
      "./jsons/travel/travelpic.json\n",
      "./jsons/travel/travelgram.json\n",
      "./jsons/travel/tourist.json\n",
      "./jsons/travel/travelislife.json\n",
      "./jsons/travel/beachvibes.json\n",
      "./jsons/travel/treking.json\n",
      "./jsons/travel/Travelawesome.json\n",
      "./jsons/travel/traveltheglobe.json\n",
      "./jsons/travel/travelworld.json\n",
      "./jsons/travel/worldtraveller.json\n",
      "./jsons/travel/Travellove.json\n",
      "./jsons/travel/nature.json\n",
      "./jsons/travel/travelcaptures.json\n",
      "./jsons/travel/Citytravel.json\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"./jsons/travel\"):\n",
    "    print(\"./jsons/travel/\" + file)\n",
    "    file_ptr = open(\"./jsons/travel/\" + file, \"r\")\n",
    "    dic = json.load(file_ptr)\n",
    "    for post in dic:\n",
    "        text = strip_punc(dic[post]['text_des'].lower())\n",
    "        toks = word_tokenize(text)\n",
    "        toks_ = []\n",
    "        for tok in toks:\n",
    "            tok = segment(tok)\n",
    "            for t in tok:\n",
    "                if not wordnet.synsets(t) or len(t) < 3:\n",
    "                    continue\n",
    "\n",
    "                t = ps.stem(t)\n",
    "                if t not in stop_words:\n",
    "                    toks_.append(t)\n",
    "            \n",
    "        if toks_:\n",
    "            glob_corpora.append(toks_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25764\n"
     ]
    }
   ],
   "source": [
    "print(len(glob_corpora))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using word2vec for training\n",
    "\n",
    "glob_model = Word2Vec(glob_corpora, min_count = 2, size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('foggi', 0.9027673602104187), ('pine', 0.9022570252418518), ('poni', 0.8992195725440979), ('snowi', 0.8893986940383911), ('leaf', 0.888798177242279), ('wood', 0.886253297328949), ('meadow', 0.8788093328475952), ('botani', 0.8753212690353394), ('classifi', 0.8740636110305786), ('protector', 0.8727027773857117), ('dusk', 0.8701772689819336), ('woodland', 0.8608576655387878), ('frosti', 0.853971004486084), ('rainbow', 0.8525438904762268), ('trailhead', 0.8525169491767883), ('glisten', 0.8508540391921997), ('mist', 0.8495299220085144), ('plant', 0.8428325653076172), ('starri', 0.842252254486084), ('vortex', 0.8400585651397705)]\n"
     ]
    }
   ],
   "source": [
    "print(glob_model.wv.most_similar(positive = 'grass', topn = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 30 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n"
     ]
    }
   ],
   "source": [
    "# Using Glove for training\n",
    "\n",
    "corpus.fit(glob_corpora, window = 10)\n",
    "glove = Glove(no_components = 300, learning_rate = 0.05)\n",
    "glove.fit(corpus.matrix, epochs = 30, no_threads = 4, verbose = True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "glove.save('glove.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'enjoy', 'travel', 'relax', 'beauti', 'view', 'india', 'natur', 'fit', 'king', 'waterfal', 'cool', 'follow']\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing of input text\n",
    "# input_sentence = \"Magestic Mountains The views which i got during my roadtrip to Heaven of Earth Kasmir still give a pleasure to my soul @holidaycompass\"\n",
    "# input_sentence = \"Straight out of The Sound of Music scenery is what you can wake up to, hiking and mountain biking your days away and enjoying Tyrolean hospitality.â \"\n",
    "input_sentence = \"Enjoy the little things in life Because someday you will look back and realize they were the BIG THINGS...\"\n",
    "# input_sentence = \"#life #enjoy #nevergiveup #bigthings #travel #travelblogger #worldtraveler #instagood #relax #fitnessmodel #beautiful #view #india #nature #instadaily #fit #modellife #king #waterfall #bodytransformation #bodypositive #cool #followforfollowback #follow\"\n",
    "text = strip_punc(input_sentence.lower())\n",
    "toks = word_tokenize(text)\n",
    "toks_ = []\n",
    "for tok in toks:\n",
    "    if wordnet.synsets(tok):\n",
    "        tok = ps.stem(tok)\n",
    "        if tok not in stop_words and len(tok)>2:\n",
    "            toks_.append(tok)\n",
    "\n",
    "input_words = toks_\n",
    "print(input_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "(10, 300)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "# Calculating sentence embedding using word2vec\n",
    "\n",
    "sent = []\n",
    "for word in input_words:\n",
    "    if word in glob_model.wv.vocab:\n",
    "        sent.append(glob_model.wv[word])\n",
    "    \n",
    "print(len(sent))\n",
    "sent = np.array(sent)\n",
    "print(sent.shape)\n",
    "embed = np.average(sent, axis = 0)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.6766943, 'memories'), (0.5459038, 'friends'), (0.5297315, 'happiness'), (0.5202689, 'smile'), (0.4357107, 'fun'), (0.42792997, 'love'), (0.40455705, 'life'), (0.40052202, 'cute'), (0.37394595, 'amazing'), (0.36331025, 'like'), (0.3442025, 'ig'), (0.32219133, 'summer'), (0.3100821, 'view'), (0.28454116, 'beauty'), (0.2819438, 'city'), (0.2796521, 'vacation'), (0.27281737, 'inspiration'), (0.26252228, 'paradise'), (0.25469896, 'sightseeing'), (0.25130293, 'exploring')]\n"
     ]
    }
   ],
   "source": [
    "# Finding similarity of top 100 hashtags using word2vec\n",
    "\n",
    "ans = []\n",
    "for hasht in top100_mapping:\n",
    "    try:\n",
    "        v1 = glob_model.wv[hasht]\n",
    "        simi = np.dot(v1, embed) / (np.linalg.norm(v1) * np.linalg.norm(embed))\n",
    "        ans.append((simi, top100_mapping[hasht]))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ans = sorted(ans, reverse = True)\n",
    "print(ans[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['memories', 'friends', 'happiness', 'smile', 'fun', 'love', 'life', 'cute', 'amazing', 'like']\n"
     ]
    }
   ],
   "source": [
    "# Finally top k for word2vec\n",
    "\n",
    "k = 10\n",
    "topk_word2vec = [h[1] for h in ans[:k]]\n",
    "print(topk_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mountain', 'view', 'got', 'heaven', 'earth', 'still', 'give', 'pleasur', 'soul']\n",
      "(9, 300)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "# Calculating sentence embedding using glove\n",
    "\n",
    "sent = []\n",
    "print(input_words)\n",
    "for word in input_words:\n",
    "    if word in glove.dictionary:\n",
    "        sent.append(glove.word_vectors[glove.dictionary[word]])\n",
    "    \n",
    "sent = np.array(sent)\n",
    "print(sent.shape)\n",
    "embed = np.average(sent, axis = 0)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.6697867535959405, 'mountain'), (0.627505397864887, 'view'), (0.6051651429506182, 'amazing'), (0.5091753441777954, 'happiness'), (0.4994858372706846, 'clouds'), (0.4982898338430924, 'beauty'), (0.4957473564542348, 'wanderlust'), (0.49444455483083094, 'love'), (0.4927563704821279, 'outdoor'), (0.4884201657783221, 'wander'), (0.4874135342249487, 'exploring'), (0.48211441994794046, 'hiking'), (0.4809235225271503, 'landscapes'), (0.48001779337101474, 'pic'), (0.4753250472503025, 'adventure'), (0.4697627488815335, 'life'), (0.464225100417928, 'discover'), (0.46414031178342136, 'green'), (0.4599611057199374, 'nature'), (0.4571901429257149, 'world')]\n"
     ]
    }
   ],
   "source": [
    "# Finding similarity using glove embeddings\n",
    "\n",
    "ans2 = []\n",
    "for hasht in top100_mapping:\n",
    "    try:\n",
    "        v1 = glove.word_vectors[glove.dictionary[hasht]]\n",
    "        simi = np.dot(v1, embed) / (np.linalg.norm(v1) * np.linalg.norm(embed))\n",
    "        ans2.append((simi, top100_mapping[hasht]))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ans2 = sorted(ans2, reverse = True)\n",
    "print(ans2[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mountain', 'view', 'amazing', 'happiness', 'clouds', 'beauty', 'wanderlust', 'love', 'outdoor', 'wander']\n"
     ]
    }
   ],
   "source": [
    "# Finally topk for glove\n",
    "\n",
    "k = 10\n",
    "\n",
    "topk_glove = [h[1] for h in ans2[:k]]\n",
    "print(topk_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mountain', 'view', 'amazing', 'happiness', 'clouds', 'beauty', 'wanderlust', 'love', 'outdoor', 'wander'] ['mountain', 'ig', 'amazing', 'forest', 'view', 'green', 'outdoor', 'hiking', 'landscapes', 'clouds']\n"
     ]
    }
   ],
   "source": [
    "print(topk_glove, topk_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on top of pre-trained 300-D word embeddings\n",
    "\n",
    "model_2 = Word2Vec(size = 300, min_count = 2)\n",
    "model_2.build_vocab(glob_corpora)\n",
    "total_examples = model_2.corpus_count\n",
    "model = KeyedVectors.load_word2vec_format(\"./glove.6B/glove.6B.300d.w2vformat.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vatsal/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3678141, 4997515)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.build_vocab([list(model.vocab.keys())], update=True)\n",
    "model_2.intersect_word2vec_format(\"./glove.6B/glove.6B.300d.w2vformat.txt\", binary=False, lockf=1.0)\n",
    "model_2.train(glob_corpora, total_examples=total_examples, epochs=model_2.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "(9, 300)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "# Calculating sentence embedding using word2vec built on top\n",
    "\n",
    "sent = []\n",
    "for word in input_words:\n",
    "    if word in model_2.wv.vocab:\n",
    "        sent.append(model_2.wv[word])\n",
    "    \n",
    "print(len(sent))\n",
    "sent = np.array(sent)\n",
    "print(sent.shape)\n",
    "embed = np.average(sent, axis = 0)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.56321794, 'mountain'), (0.49063486, 'view'), (0.4691943, 'amazing'), (0.41959488, 'landscapes'), (0.41020262, 'clouds'), (0.40212187, 'love'), (0.40068957, 'beauty'), (0.39671206, 'exploring'), (0.39055562, 'nature'), (0.37776214, 'adventure'), (0.3502084, 'life'), (0.33659765, 'sky'), (0.33638433, 'hiking'), (0.33566836, 'forest'), (0.335485, 'discover'), (0.32776678, 'green'), (0.3153249, 'sunrise'), (0.31120405, 'world'), (0.3103188, 'inspiration'), (0.29969543, 'india')]\n"
     ]
    }
   ],
   "source": [
    "# Finding similarity of top 100 hashtags using word2vec built on top\n",
    "\n",
    "ans3 = []\n",
    "for hasht in top100_mapping:\n",
    "    try:\n",
    "        v1 = model_2.wv[hasht]\n",
    "        simi = np.dot(v1, embed) / (np.linalg.norm(v1) * np.linalg.norm(embed))\n",
    "        ans3.append((simi, top100_mapping[hasht]))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ans3 = sorted(ans3, reverse = True)\n",
    "print(ans3[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mountain', 'view', 'amazing', 'landscapes', 'clouds', 'love', 'beauty', 'exploring', 'nature', 'adventure'] ['mountain', 'ig', 'amazing', 'forest', 'view', 'green', 'outdoor', 'hiking', 'landscapes', 'clouds']\n"
     ]
    }
   ],
   "source": [
    "topk_word2vec_on_top = [h[1] for h in ans3[:k]] \n",
    "print(topk_word2vec_on_top, topk_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input  Magestic Mountains The views which i got during my roadtrip to Heaven of Earth Kasmir still give a pleasure to my soul @holidaycompass\n",
      "Glove output\n",
      "  ['mountain', 'view', 'amazing', 'happiness', 'clouds', 'beauty', 'wanderlust', 'love', 'outdoor', 'wander']\n",
      "Word2Vec output\n",
      "  ['mountain', 'ig', 'amazing', 'forest', 'view', 'green', 'outdoor', 'hiking', 'landscapes', 'clouds']\n",
      "Word2Vec output with training on top of pre-trained\n",
      "  ['mountain', 'view', 'amazing', 'landscapes', 'clouds', 'love', 'beauty', 'exploring', 'nature', 'adventure']\n"
     ]
    }
   ],
   "source": [
    "print(\"Input \", input_sentence)\n",
    "print(\"Glove output\\n \", topk_glove)\n",
    "print(\"Word2Vec output\\n \", topk_word2vec)\n",
    "print(\"Word2Vec output with training on top of pre-trained\\n \", topk_word2vec_on_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('philippin', 0.6941967606544495), ('holiday', 0.6935321092605591), ('summer', 0.6657976508140564), ('wednesday', 0.6657053232192993), ('happi', 0.650692343711853), ('cebu', 0.6470319628715515), ('good', 0.6442487239837646), ('smile', 0.6439228057861328), ('memori', 0.6433659791946411), ('flirt', 0.6238188743591309)]\n",
      "[('philippin', 0.48417752981185913), ('good', 0.457288920879364), ('colleagu', 0.4232737421989441), ('holiday', 0.4217432737350464), ('love', 0.4049089550971985), ('funni', 0.3766258955001831), ('summer', 0.3744525909423828), ('cute', 0.37307867407798767), ('happi', 0.37076669931411743), ('vacat', 0.3666195571422577)]\n"
     ]
    }
   ],
   "source": [
    "test = \"fun\"\n",
    "# test = \"grass\"\n",
    "print(glob_model.wv.most_similar(positive = test, topn = 10))\n",
    "print(model_2.wv.most_similar(positive = test, topn = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9338\n"
     ]
    }
   ],
   "source": [
    "print(len(list(model_2.wv.vocab.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "origs = ['life', 'enjoy', 'travel', 'relax', 'beauti', 'view', 'india', 'natur', 'fit', 'king', 'waterfal', 'cool', 'follow']\n",
    "preds = ['memories', 'friends', 'happiness', 'smile', 'fun', 'love', 'life', 'cute', 'amazing', 'like']\n",
    "\n",
    "\n",
    "matr = np.empty((len(preds), len(origs)))\n",
    "\n",
    "for i, pred in enumerate(preds):\n",
    "    for j, orig in enumerate(origs):\n",
    "        try:\n",
    "            similarity = model_2.wv.similarity(ps.stem(orig), ps.stem(pred))\n",
    "        except:\n",
    "            similarity = 0\n",
    "        matr[i][j] = similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memories-------------life\n",
      "friends-------------follow\n",
      "happiness-------------life\n",
      "smile-------------fit\n",
      "fun-------------cool\n",
      "love-------------travel\n",
      "life-------------life\n",
      "cute-------------cool\n",
      "amazing-------------beauti\n",
      "like-------------follow\n",
      "+-----------+------+-------+--------+-------+--------+------+-------+-------+------+-------+----------+------+--------+\n",
      "| Predicted | life | enjoy | travel | relax | beauti | view | india | natur | fit  |  king | waterfal | cool | follow |\n",
      "+-----------+------+-------+--------+-------+--------+------+-------+-------+------+-------+----------+------+--------+\n",
      "|  memories | 0.36 |  0.31 |  0.31  |  0.22 |  0.26  | 0.12 |  0.11 |  0.13 | 0.04 | -0.01 |   0.0    | 0.17 |  0.12  |\n",
      "|  friends  | 0.17 |  0.32 |  0.11  |  0.22 |  0.15  | 0.15 |  0.1  |  0.1  | 0.15 |  0.18 |   0.0    | 0.24 |  0.35  |\n",
      "| happiness | 0.37 |  0.36 |  0.25  |  0.3  |  0.32  | 0.17 |  0.15 |  0.21 | 0.27 |  0.11 |   0.0    | 0.27 |  0.32  |\n",
      "|   smile   | 0.14 |  0.24 |  0.05  |  0.22 |  0.21  | 0.01 |  0.01 |  0.08 | 0.34 |  0.05 |   0.0    | 0.29 |  0.17  |\n",
      "|    fun    | 0.32 |  0.3  |  0.26  |  0.3  |  0.27  | 0.12 |  0.14 |  0.16 | 0.25 |  0.11 |   0.0    | 0.32 |  0.2   |\n",
      "|    love   | 0.5  |  0.27 |  0.53  |  0.18 |  0.47  | 0.28 |  0.29 |  0.41 | 0.19 |  0.12 |   0.0    | 0.26 |  0.33  |\n",
      "|    life   | 1.0  |  0.19 |  0.59  |  0.14 |  0.31  | 0.3  |  0.33 |  0.4  | 0.26 |  0.03 |   0.0    | 0.17 |  0.29  |\n",
      "|    cute   | 0.13 |  0.12 |  0.01  |  0.12 |  0.21  | 0.08 | -0.03 |  0.09 | 0.28 |  0.03 |   0.0    | 0.46 |  0.23  |\n",
      "|  amazing  | 0.28 |  0.25 |  0.34  |  0.19 |  0.56  | 0.39 |  0.25 |  0.38 | 0.13 |  0.19 |   0.0    | 0.35 |  0.41  |\n",
      "|    like   | 0.26 |  0.25 |  0.33  |  0.16 |  0.27  | 0.19 |  0.2  |  0.21 | 0.23 |  0.11 |   0.0    | 0.36 |  0.55  |\n",
      "+-----------+------+-------+--------+-------+--------+------+-------+-------+------+-------+----------+------+--------+\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "# print(matr)\n",
    "accs = []\n",
    "from prettytable import PrettyTable\n",
    "tabl = PrettyTable()\n",
    "tabl.field_names = ['Predicted'] + origs\n",
    "for ind, row in enumerate(matr):\n",
    "    tabl.add_row([preds[ind]] + [round(i,2) for i in row])\n",
    "    accs.append(max(row))\n",
    "    indi = np.argmax(row)\n",
    "    print(preds[ind] + \"-------------\" + origs[indi])\n",
    "print(tabl)\n",
    "# print(accs)\n",
    "# print(np.mean(accs))\n",
    "\n",
    "ans = 0\n",
    "for acc in accs:\n",
    "    if round(acc,1) >= 0.4:\n",
    "        ans += 1\n",
    "print(ans / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prettytable\n",
      "Installing collected packages: prettytable\n",
      "Successfully installed prettytable-0.7.2\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install prettytable --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
